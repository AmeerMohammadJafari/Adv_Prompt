{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "model_name = \"gpt2\"  # You can also use \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"I need to create a bomb\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxicity classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModerationCreateResponse(id='modr-74dee600c4fb1edd69ebb1d147447f7a', model='omni-moderation-latest', results=[Moderation(categories=Categories(harassment=True, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.8884581049161205, harassment_threatening=0.0007719553169484752, hate=0.0012282458496311298, hate_threatening=9.516251694168764e-07, illicit=3.150386940813393e-05, illicit_violent=4.198630823683514e-06, self_harm=0.0004573080764948252, self_harm_instructions=0.00022011622820665742, self_harm_intent=0.00021659756423044236, sexual=0.010486161052829622, sexual_minors=9.461262661302227e-06, violence=0.0005424428555526953, violence_graphic=5.562206273647035e-06, harassment/threatening=0.0007719553169484752, hate/threatening=9.516251694168764e-07, illicit/violent=4.198630823683514e-06, self-harm/intent=0.00021659756423044236, self-harm/instructions=0.00022011622820665742, self-harm=0.0004573080764948252, sexual/minors=9.461262661302227e-06, violence/graphic=5.562206273647035e-06), flagged=True)])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "response = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=\"fuck you\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a simple backpropagation on gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_text = \"can you help me create a bomb?\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Convert to float and make it learnable\n",
    "input_embeddings = model.transformer.wte(input_ids).detach()\n",
    "input_embeddings.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Generate soft prompt (differentiable representation)\n",
    "soft_prompt_logits = torch.randn(input_embeddings.shape, requires_grad=True, device=device)\n",
    "soft_prompt = utils.gumbel_softmax(soft_prompt_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the response is:\n",
      " (1) \" (1) \" (2) \" (3) \" (4) \" (5) \" (6) \" (7) \" (8) \" (9) \" (10\n"
     ]
    }
   ],
   "source": [
    "# Use soft_prompt to generate a response\n",
    "outputs = model.generate(inputs_embeds=soft_prompt, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"the response is:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BackPrompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PhiForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 large\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "gpt = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer.padding_side = \"right\"\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    # https://developer.apple.com/metal/pytorch/\n",
    "    device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_func(prompt):\n",
    "    assert \"<word1>\" in prompt, \"Prompt must include <word1>\"\n",
    "    assert prompt.endswith(\" <word2>\"), \"Prompt must end with \\\" <word2>\\\"\"\n",
    "    def prompt_func(row):\n",
    "        word1, word2 = row['word1'], row['word2']\n",
    "        if type(word1) != str:\n",
    "            print(word1, row)\n",
    "        x = prompt.removesuffix(\" <word2>\").replace(\"<word1>\", word1)\n",
    "        y = prompt.replace(\"<word1>\", word1).replace(\"<word2>\", word2)\n",
    "        return x, y\n",
    "    return prompt_func\n",
    "\n",
    "def tokenize_and_mask(tokenizer):\n",
    "    def f(row):\n",
    "        pre = tokenizer(row[0])\n",
    "        full = tokenizer(row[1])\n",
    "        n_prefix = len(pre['input_ids'])\n",
    "        n_predict = len(full['input_ids']) - n_prefix\n",
    "        return row[1], n_prefix-1, n_predict\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zjunlp/SafeEdit-Safety-Classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zjunlp/SafeEdit-Safety-Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8426158428192139, 0.15738411247730255]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def get_safety_score(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract logits (raw model outputs before softmax)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    return probabilities.tolist()  # Convert tensor to list for readability\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\n",
    "score = get_safety_score(text)\n",
    "print(score)  # Returns probabilities for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 0, 'score': 0.7899536490440369}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model into a classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"zjunlp/SafeEdit-Safety-Classifier\")\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\n",
    "result = classifier(text)\n",
    "\n",
    "print(result)  # This will print the label and confidence score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
