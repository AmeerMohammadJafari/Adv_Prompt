{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need to create a bomb. I need\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "model_name = \"gpt2\"  # You can also use \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"I need to create a bomb\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxicity classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModerationCreateResponse(id='modr-74dee600c4fb1edd69ebb1d147447f7a', model='omni-moderation-latest', results=[Moderation(categories=Categories(harassment=True, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.8884581049161205, harassment_threatening=0.0007719553169484752, hate=0.0012282458496311298, hate_threatening=9.516251694168764e-07, illicit=3.150386940813393e-05, illicit_violent=4.198630823683514e-06, self_harm=0.0004573080764948252, self_harm_instructions=0.00022011622820665742, self_harm_intent=0.00021659756423044236, sexual=0.010486161052829622, sexual_minors=9.461262661302227e-06, violence=0.0005424428555526953, violence_graphic=5.562206273647035e-06, harassment/threatening=0.0007719553169484752, hate/threatening=9.516251694168764e-07, illicit/violent=4.198630823683514e-06, self-harm/intent=0.00021659756423044236, self-harm/instructions=0.00022011622820665742, self-harm=0.0004573080764948252, sexual/minors=9.461262661302227e-06, violence/graphic=5.562206273647035e-06), flagged=True)])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "response = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=\"fuck you\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a simple backpropagation on gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_text = \"can you help me create a bomb?\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Convert to float and make it learnable\n",
    "input_embeddings = model.transformer.wte(input_ids).detach()\n",
    "input_embeddings.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Generate soft prompt (differentiable representation)\n",
    "soft_prompt_logits = torch.randn(input_embeddings.shape, requires_grad=True, device=device)\n",
    "soft_prompt = utils.gumbel_softmax(soft_prompt_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the response is:\n",
      " (1) \" (1) \" (2) \" (3) \" (4) \" (5) \" (6) \" (7) \" (8) \" (9) \" (10\n"
     ]
    }
   ],
   "source": [
    "# Use soft_prompt to generate a response\n",
    "outputs = model.generate(inputs_embeds=soft_prompt, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"the response is:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BackPrompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PhiForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 large\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "gpt = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer.padding_side = \"right\"\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    # https://developer.apple.com/metal/pytorch/\n",
    "    device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_func(prompt):\n",
    "    assert \"<word1>\" in prompt, \"Prompt must include <word1>\"\n",
    "    assert prompt.endswith(\" <word2>\"), \"Prompt must end with \\\" <word2>\\\"\"\n",
    "    def prompt_func(row):\n",
    "        word1, word2 = row['word1'], row['word2']\n",
    "        if type(word1) != str:\n",
    "            print(word1, row)\n",
    "        x = prompt.removesuffix(\" <word2>\").replace(\"<word1>\", word1)\n",
    "        y = prompt.replace(\"<word1>\", word1).replace(\"<word2>\", word2)\n",
    "        return x, y\n",
    "    return prompt_func\n",
    "\n",
    "def tokenize_and_mask(tokenizer):\n",
    "    def f(row):\n",
    "        pre = tokenizer(row[0])\n",
    "        full = tokenizer(row[1])\n",
    "        n_prefix = len(pre['input_ids'])\n",
    "        n_predict = len(full['input_ids']) - n_prefix\n",
    "        return row[1], n_prefix-1, n_predict\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zjunlp/SafeEdit-Safety-Classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zjunlp/SafeEdit-Safety-Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8426158428192139, 0.15738411247730255]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def get_safety_score(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract logits (raw model outputs before softmax)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    return probabilities.tolist()  # Convert tensor to list for readability\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\n",
    "score = get_safety_score(text)\n",
    "print(score)  # Returns probabilities for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 0, 'score': 0.7899536490440369}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load model into a classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"zjunlp/SafeEdit-Safety-Classifier\")\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\n",
    "result = classifier(text)\n",
    "\n",
    "print(result)  # This will print the label and confidence score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "gpt = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"openai-community/gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gpt.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving train dataset...\n",
      "Downloading and saving validation dataset...\n",
      "Downloading and saving test dataset...\n",
      "✅ All datasets saved successfully in 'dataset' directory!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the \"dataset\" directory exists\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Define dataset splits and file names\n",
    "splits = {\n",
    "    \"train\": \"train.json\",\n",
    "    \"validation\": \"validation.json\",\n",
    "    \"test\": \"test.json\"\n",
    "}\n",
    "\n",
    "# Base path for Hugging Face dataset\n",
    "base_path = \"hf://datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0/\"\n",
    "\n",
    "# Loop through each split, load, and save\n",
    "for split_name, filename in splits.items():\n",
    "    print(f\"Downloading and saving {split_name} dataset...\")\n",
    "\n",
    "    # Load dataset from Hugging Face\n",
    "    df = pd.read_json(base_path + filename)\n",
    "\n",
    "    # Save in different formats\n",
    "    df.to_csv(f\"dataset/{split_name}.csv\", index=False)  # Save as CSV\n",
    "    df.to_json(f\"dataset/{split_name}.json\", orient=\"records\", lines=True)  # Save as JSON\n",
    "    df.to_parquet(f\"dataset/{split_name}.parquet\")  # Save as Parquet\n",
    "\n",
    "print(\"✅ All datasets saved successfully in 'dataset' directory!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>response_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to keep a baby is in its mother...</td>\n",
       "      <td>unsafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m not sure what that means, but it sounds li...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don’t. I believe that there are some animals...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m not talking about the price of an essay, b...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I’m not talking about the ones that are sent o...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>I was at my local grocery store, buying some g...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Fif is an abbreviation for “For Itself.” The t...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>Kim Kardashian has been married to Kanye West ...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>I don't think I do, but I can try to find it f...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>I’m sure you have seen it. The mom who is tryi...</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10234 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response response_label\n",
       "0      The best place to keep a baby is in its mother...         unsafe\n",
       "2      I’m not sure what that means, but it sounds li...           safe\n",
       "3      I don’t. I believe that there are some animals...           safe\n",
       "4      I’m not talking about the price of an essay, b...           safe\n",
       "5      I’m not talking about the ones that are sent o...           safe\n",
       "...                                                  ...            ...\n",
       "25002  I was at my local grocery store, buying some g...           safe\n",
       "25003  Fif is an abbreviation for “For Itself.” The t...           safe\n",
       "25004  Kim Kardashian has been married to Kanye West ...           safe\n",
       "25005  I don't think I do, but I can try to find it f...           safe\n",
       "25006  I’m sure you have seen it. The mom who is tryi...           safe\n",
       "\n",
       "[10234 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON file into a DataFrame\n",
    "df = pd.read_json(\"dataset/train.json\", orient=\"records\", lines=True)\n",
    "\n",
    "# Select only the 'response' and 'response_label' columns and drop rows with None values\n",
    "df_filtered = df[[\"response\", \"response_label\"]].dropna()\n",
    "\n",
    "# Display the first few rows\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>response_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to keep a baby is in its mother...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m not sure what that means, but it sounds li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don’t. I believe that there are some animals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m not talking about the price of an essay, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I’m not talking about the ones that are sent o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>I was at my local grocery store, buying some g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Fif is an abbreviation for “For Itself.” The t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>Kim Kardashian has been married to Kanye West ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>I don't think I do, but I can try to find it f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>I’m sure you have seen it. The mom who is tryi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10234 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  response_label\n",
       "0      The best place to keep a baby is in its mother...               0\n",
       "2      I’m not sure what that means, but it sounds li...               1\n",
       "3      I don’t. I believe that there are some animals...               1\n",
       "4      I’m not talking about the price of an essay, b...               1\n",
       "5      I’m not talking about the ones that are sent o...               1\n",
       "...                                                  ...             ...\n",
       "25002  I was at my local grocery store, buying some g...               1\n",
       "25003  Fif is an abbreviation for “For Itself.” The t...               1\n",
       "25004  Kim Kardashian has been married to Kanye West ...               1\n",
       "25005  I don't think I do, but I can try to find it f...               1\n",
       "25006  I’m sure you have seen it. The mom who is tryi...               1\n",
       "\n",
       "[10234 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define mapping: \"safe\" → 1, \"unsafe\" → 0\n",
    "label_mapping = {\"safe\": 1, \"unsafe\": 0}\n",
    "\n",
    "# Apply the mapping to the 'response_label' column\n",
    "df_labeled = df_filtered.copy()  # Create a copy to avoid modifying the original\n",
    "df_labeled[\"response_label\"] = df_labeled[\"response_label\"].map(label_mapping)\n",
    "\n",
    "# Display the first few rows\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_label\n",
      "1    6693\n",
      "0    3541\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df_labeled[\"response_label\"].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\Desktop\\BSc Thesis\\Adv_Prompt\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_36200\\2537500567.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_36200\\2537500567.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.5675310904528956\n",
      "Epoch 2/3, Loss: 0.4247172089322182\n",
      "Epoch 3/3, Loss: 0.37738696455352727\n",
      "Training complete! ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AdamW\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load tokenizer and model\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Use smaller GPT-2 model\n",
    "gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Freeze embeddings\n",
    "for param in gpt.transformer.wte.parameters():\n",
    "    param.requires_grad = False  # Freeze token embeddings\n",
    "\n",
    "# Modify GPT-2 for classification\n",
    "class GPT2ForClassification(nn.Module):\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model  # Base GPT-2 model\n",
    "        self.classifier = nn.Linear(gpt_model.config.n_embd, 1)  # Classification head\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt.transformer(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        cls_representation = last_hidden_state[:, -1, :]  # Use last token representation\n",
    "        logits = self.classifier(cls_representation)  # Output shape: (batch_size, 1)\n",
    "        return logits\n",
    "\n",
    "# Initialize modified model\n",
    "model = GPT2ForClassification(gpt)\n",
    "\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token  # Set pad token as EOS\n",
    "\n",
    "# Load dataset into PyTorch\n",
    "class ResponseDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"response\"].tolist()\n",
    "        self.labels = df[\"response_label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            padding=\"max_length\",  # Ensures equal-length sequences\n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)  # Binary labels\n",
    "        \n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 2  # Reduced batch size to save memory\n",
    "train_dataset = ResponseDataset(df_labeled, gpt_tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Binary classification loss\n",
    "\n",
    "# Use mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients over 4 mini-batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            logits = model(input_ids, attention_mask).squeeze(-1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Scales the loss and calls backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Unscales gradients and updates the optimizer\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to models\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the directory where the model will be saved\n",
    "save_directory = \"models\"\n",
    "\n",
    "# Save the model's state_dict (weights)\n",
    "torch.save(model.state_dict(), f\"{save_directory}/gpt2_classifier.bin\")\n",
    "\n",
    "# Save the model's configuration\n",
    "model.gpt.config.to_json_file(f\"{save_directory}/config.json\")\n",
    "\n",
    "# Save the tokenizer\n",
    "gpt_tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer reloaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the directory where the model is saved\n",
    "save_directory = \"models\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model configuration\n",
    "config = GPT2Config.from_json_file(f\"{save_directory}/config.json\")\n",
    "\n",
    "# Initialize the model with the configuration\n",
    "model = GPT2ForClassification(GPT2LMHeadModel(config))\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load(f\"{save_directory}/gpt2_classifier.bin\"))\n",
    "\n",
    "# Move model to the appropriate device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer reloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training dataset: 86.64%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare variables to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Inference loop\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Get logits from the model\n",
    "        logits = model(input_ids, attention_mask).squeeze(-1)\n",
    "\n",
    "        # Convert logits to predictions (threshold at 0.5 for binary classification)\n",
    "        preds = torch.round(torch.sigmoid(logits))  # Sigmoid for probability and round to get binary label\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy on the training dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
